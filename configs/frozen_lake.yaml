training:
  log_interval: 100
  max_steps_per_episode: 300
  num_episodes: 5000
  save_interval: 500
  speed: 10000000

testing:
  num_episodes: 4000
  speed: 10000000

reward:
  goal: 4.0
  hole: -3.0
  wall: -0.1
  ice: -0.05
  unsafe: -0.2

agent:
  # DDQN or PPO
  rl_type: DDQN

  # MLP or CNN
  model_type: CNN

  #ddqn specific
  epsilon_decay: 3000
  epsilon_end: 0.01
  epsilon_start: 1.0
  target_update_freq: 10
  gamma_ddqn: 0.85
  learning_rate_ddqn: 0.0005

  # PPO-specific
  clip_epsilon: 0.2
  ppo_epochs: 6
  entropy_coeff: 0.005
  gae_lambda: 0.95
  gamma_ppo: 0.99
  learning_rate_ppo: 0.0003

  mlp:
    hidden_size: 128

  cnn:
    hidden_size: 512
    input_shape: [4, 6, 6] #[channels, height, width]
    conv_channels: 32

env:
  is_slippery: true
  map_size: 6
  goal: 0.7
  hole: 0
  ice: 0
  agent: 1

memory:
  batch_size: 128
  buffer_size: 50000